{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f81bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 tickers: ['ABBV', 'AMGN', 'AZN', 'BIIB', 'BMY', 'GILD', 'GSK', 'JNJ', 'LLY', 'MRK']...\n",
      "OLS R2 train: 0.008 | test: -0.019\n",
      "RF  R2 train: 0.038 | test: -0.010\n",
      "RF grid best: {'max_depth': 5, 'max_features': 2, 'min_samples_leaf': 5, 'min_samples_split': 15, 'n_estimators': 300} | R2 test: -0.010741192030550906\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'J_7'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 274\u001b[0m\n\u001b[0;32m    272\u001b[0m tmp \u001b[38;5;241m=\u001b[39m ensure_date_column(data_test)\n\u001b[0;32m    273\u001b[0m date_col \u001b[38;5;241m=\u001b[39m find_date_col(tmp)\n\u001b[1;32m--> 274\u001b[0m MOM_7 \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39mdate_col, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ_7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m MOM_7_frame, MOM_7_cum, MOM_7_profit \u001b[38;5;241m=\u001b[39m MOM_Profit(MOM_7, K_HOLD, daily_close_px)\n\u001b[0;32m    277\u001b[0m RF_frame,  RF_cum,  RF_profit       \u001b[38;5;241m=\u001b[39m MOM_Profit(RF_results, K_HOLD, daily_close_px)\n",
      "File \u001b[1;32mc:\\Users\\cooki\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9509\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9492\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9493\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   9494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9505\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   9506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9507\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 9509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot_table(\n\u001b[0;32m   9510\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9511\u001b[0m         values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[0;32m   9512\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   9513\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   9514\u001b[0m         aggfunc\u001b[38;5;241m=\u001b[39maggfunc,\n\u001b[0;32m   9515\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m   9516\u001b[0m         margins\u001b[38;5;241m=\u001b[39mmargins,\n\u001b[0;32m   9517\u001b[0m         dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   9518\u001b[0m         margins_name\u001b[38;5;241m=\u001b[39mmargins_name,\n\u001b[0;32m   9519\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   9520\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9521\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\cooki\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m __internal_pivot_table(\n\u001b[0;32m    103\u001b[0m     data,\n\u001b[0;32m    104\u001b[0m     values,\n\u001b[0;32m    105\u001b[0m     index,\n\u001b[0;32m    106\u001b[0m     columns,\n\u001b[0;32m    107\u001b[0m     aggfunc,\n\u001b[0;32m    108\u001b[0m     fill_value,\n\u001b[0;32m    109\u001b[0m     margins,\n\u001b[0;32m    110\u001b[0m     dropna,\n\u001b[0;32m    111\u001b[0m     margins_name,\n\u001b[0;32m    112\u001b[0m     observed,\n\u001b[0;32m    113\u001b[0m     sort,\n\u001b[0;32m    114\u001b[0m )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cooki\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:148\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(i)\n\u001b[0;32m    150\u001b[0m to_filter \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;241m+\u001b[39m values:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'J_7'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Random-Forest predictor on local by_ticker_csv (index/date safe)\n",
    "# ============================================\n",
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DATA_DIR = r\"C:\\Users\\cooki\\OneDrive\\Uni\\Seminar Econ in financial application\\by_ticker_csv\"\n",
    "TICKERS = ['ABBV', 'AMGN', 'AZN', 'BIIB', 'BMY', 'GILD', 'GSK',\n",
    "           'JNJ', 'LLY', 'MRK', 'NVO', 'NVS', 'PFE', 'RHHBY', 'SNY']\n",
    "START = datetime.datetime(2015, 1, 2)\n",
    "END   = datetime.datetime(2024, 12, 31)\n",
    "K_HOLD   = 7             # holding period used for momentum evaluation\n",
    "LOOKBACKS = (3,5,15,30)  # momentum feature lengths\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers to keep 'Date' robust\n",
    "# ----------------------------\n",
    "def ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a copy of df where a datetime column named 'Date' exists,\n",
    "    whether the date was an index (named/unnamed) or already a column.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if 'Date' in out.columns and is_datetime64_any_dtype(out['Date']):\n",
    "        return out\n",
    "    # If index is datetime -> move to a column\n",
    "    if is_datetime64_any_dtype(out.index):\n",
    "        out = out.reset_index()\n",
    "        # After reset, the datetime column may be named 'index' or original index name\n",
    "        dt_cols = [c for c in out.columns if is_datetime64_any_dtype(out[c])]\n",
    "        if dt_cols:\n",
    "            if 'Date' not in dt_cols:\n",
    "                out = out.rename(columns={dt_cols[0]: 'Date'})\n",
    "            return out\n",
    "    # If any datetime column exists, rename first to 'Date'\n",
    "    dt_cols = [c for c in out.columns if is_datetime64_any_dtype(out[c])]\n",
    "    if dt_cols:\n",
    "        if 'Date' not in dt_cols:\n",
    "            out = out.rename(columns={dt_cols[0]: 'Date'})\n",
    "        return out\n",
    "    raise KeyError(\"No datetime column found to use as 'Date'.\")\n",
    "\n",
    "def find_date_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return the datetime column name to use as pivot index.\"\"\"\n",
    "    if 'Date' in df.columns and is_datetime64_any_dtype(df['Date']):\n",
    "        return 'Date'\n",
    "    for c in df.columns:\n",
    "        if is_datetime64_any_dtype(df[c]):\n",
    "            return c\n",
    "    raise KeyError(\"No datetime column found.\")\n",
    "\n",
    "# ----------------------------\n",
    "# IO helpers\n",
    "# ----------------------------\n",
    "def list_tickers(path):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {path}\")\n",
    "    return sorted(os.path.splitext(f)[0] for f in os.listdir(path) if f.lower().endswith(\".csv\"))\n",
    "\n",
    "def load_one(ticker):\n",
    "    p = os.path.join(DATA_DIR, f\"{ticker}.csv\")\n",
    "    if not os.path.isfile(p): raise FileNotFoundError(p)\n",
    "    df = pd.read_csv(p, parse_dates=[\"Date\"]).sort_values(\"Date\")\n",
    "    if \"Adj Close\" not in df.columns:\n",
    "        if \"Close\" not in df.columns:\n",
    "            raise ValueError(f\"{ticker}: missing Close/Adj Close\")\n",
    "        df[\"Adj Close\"] = df[\"Close\"]\n",
    "    if \"Volume\" not in df.columns:\n",
    "        df[\"Volume\"] = np.nan\n",
    "    df[\"Ticker\"] = ticker\n",
    "    return df[[\"Date\",\"Ticker\",\"Adj Close\",\"Volume\"]]\n",
    "\n",
    "def get_data(tickers, start=None, end=None):\n",
    "    frames = []\n",
    "    for t in tickers:\n",
    "        df = load_one(t)\n",
    "        if start: df = df[df[\"Date\"] >= pd.Timestamp(start)]\n",
    "        if end:   df = df[df[\"Date\"] <= pd.Timestamp(end)]\n",
    "        frames.append(df)\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Feature engineering (index-safe using transform)\n",
    "# ----------------------------\n",
    "def prep_data(tickers, start=None, end=None, K=7):\n",
    "    raw = get_data(tickers, start, end)\n",
    "\n",
    "    # Wide price and returns\n",
    "    px = raw.pivot(index=\"Date\", columns=\"Ticker\", values=\"Adj Close\").sort_index()\n",
    "    ret = px.pct_change()\n",
    "\n",
    "    # Long per-asset table\n",
    "    d = raw.rename(columns={\"Adj Close\":\"Price\"}).copy()\n",
    "\n",
    "    # daily reversal (t-1) — aligned\n",
    "    d[\"t-1\"] = d.groupby(\"Ticker\")[\"Price\"].pct_change().shift(1)\n",
    "\n",
    "    # momentum & volatility windows (use transform to keep original index)\n",
    "    for L in LOOKBACKS:\n",
    "        d[f\"J_{L}\"] = d.groupby(\"Ticker\")[\"t-1\"].transform(\n",
    "            lambda s: (1 + s).rolling(L).apply(np.prod, raw=True) - 1\n",
    "        )\n",
    "        d[f\"J_std_{L}\"] = d.groupby(\"Ticker\")[\"t-1\"].transform(\n",
    "            lambda s: (1 + s).rolling(L).std()\n",
    "        )\n",
    "\n",
    "    # forward K-day target — aligned\n",
    "    d[\"target\"] = d.groupby(\"Ticker\")[\"Price\"].shift(-K) / d[\"Price\"] - 1\n",
    "\n",
    "    # clean\n",
    "    d = d.dropna().sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # index by Date for splitting\n",
    "    d_idx = d.set_index(\"Date\")\n",
    "    d_idx.index.name = \"Date\"  # keep name to make reset_index stable\n",
    "    return d, d_idx, ret, px\n",
    "\n",
    "# ----------------------------\n",
    "# Chronological split 80/20\n",
    "# ----------------------------\n",
    "def split_train_test(d_idx, px, test_ratio=0.2):\n",
    "    common_dates = pd.Index(sorted(set(d_idx.index) & set(px.index)))\n",
    "    if common_dates.empty:\n",
    "        raise ValueError(\"No overlapping dates between features and prices.\")\n",
    "    cut = int(len(common_dates) * (1 - test_ratio)) or 1\n",
    "    dates_train = common_dates[:cut]\n",
    "    dates_test  = common_dates[cut:]\n",
    "\n",
    "    train = d_idx.loc[dates_train]\n",
    "    test  = d_idx.loc[dates_test]\n",
    "\n",
    "    data_train = train.copy()\n",
    "    data_test  = test.copy()\n",
    "\n",
    "    drop_cols = [\"Ticker\",\"Price\",\"target\"]\n",
    "    X_train = (train.drop(columns=drop_cols, errors=\"ignore\")\n",
    "                    .select_dtypes(include=[np.number])\n",
    "                    .replace([np.inf,-np.inf], np.nan).fillna(0.0))\n",
    "    y_train = train[\"target\"].copy()\n",
    "    X_test  = (test.drop(columns=drop_cols, errors=\"ignore\")\n",
    "                   .select_dtypes(include=[np.number])\n",
    "                   .replace([np.inf,-np.inf], np.nan).fillna(0.0))\n",
    "    y_test  = test[\"target\"].copy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, data_train, data_test\n",
    "\n",
    "# ----------------------------\n",
    "# Momentum helpers (for comparison)\n",
    "# ----------------------------\n",
    "def Momentum(pred_r, date, K, px):\n",
    "    ret = pred_r.loc[date].reset_index()\n",
    "    ret[\"quantile\"] = pd.qcut(ret.iloc[:,1].rank(method=\"first\"), 3, labels=False)\n",
    "    winners = ret[ret[\"quantile\"] == 2]\n",
    "    losers  = ret[ret[\"quantile\"] == 0]\n",
    "    next_date = date + relativedelta(days=K)\n",
    "    if next_date not in px.index:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    w = px.loc[next_date, px.columns.isin(winners.Ticker)] / px.loc[date, px.columns.isin(winners.Ticker)] - 1\n",
    "    l = px.loc[next_date, px.columns.isin(losers.Ticker)]  / px.loc[date, px.columns.isin(losers.Ticker)]  - 1\n",
    "    mom = w.mean() - l.mean()\n",
    "    return (mom / K * 7), (w.mean() / K * 7), (-l.mean() / K * 7)\n",
    "\n",
    "def MOM_Profit(returns, K, px):\n",
    "    profits, wins, loss, dates = [], [], [], []\n",
    "    for date in returns.index[:-K]:\n",
    "        m, win, los = Momentum(returns, date, K, px)\n",
    "        if np.isnan(m): continue\n",
    "        profits.append(m); wins.append(win); loss.append(los); dates.append(date)\n",
    "    frame = pd.DataFrame({\"MomentumProfit\": profits, \"Winners\": wins, \"Losers\": loss}, index=dates) * 100\n",
    "    return frame, frame.cumsum(), frame.MomentumProfit.sum()\n",
    "\n",
    "# ----------------------------\n",
    "# Robust prediction -> panel converter (no index alignment issues)\n",
    "# ----------------------------\n",
    "def attach_preds(model, X_train, X_test, data_train, data_test, name):\n",
    "    y_pred_train = np.asarray(model.predict(X_train)).reshape(-1)\n",
    "    y_pred_test  = np.asarray(model.predict(X_test)).reshape(-1)\n",
    "\n",
    "    tr = ensure_date_column(data_train)\n",
    "    dt = ensure_date_column(data_test)\n",
    "    date_col_tr = find_date_col(tr)\n",
    "    date_col_dt = find_date_col(dt)\n",
    "\n",
    "    if len(tr) != len(y_pred_train):\n",
    "        raise ValueError(f\"[{name}] train rows ({len(tr)}) != train preds ({len(y_pred_train)})\")\n",
    "    if len(dt) != len(y_pred_test):\n",
    "        raise ValueError(f\"[{name}] test rows ({len(dt)}) != test preds ({len(y_pred_test)})\")\n",
    "\n",
    "    tr_pred = pd.DataFrame({\n",
    "        \"Date\":   tr[date_col_tr].to_numpy(),\n",
    "        \"Ticker\": tr[\"Ticker\"].to_numpy(),\n",
    "        f\"{name}_pred\": y_pred_train\n",
    "    })\n",
    "    dt_pred = pd.DataFrame({\n",
    "        \"Date\":   dt[date_col_dt].to_numpy(),\n",
    "        \"Ticker\": dt[\"Ticker\"].to_numpy(),\n",
    "        f\"{name}_pred\": y_pred_test\n",
    "    })\n",
    "\n",
    "    panel_train = tr_pred.pivot_table(index=\"Date\", columns=\"Ticker\", values=f\"{name}_pred\")\n",
    "    panel_test  = dt_pred.pivot_table(index=\"Date\", columns=\"Ticker\", values=f\"{name}_pred\")\n",
    "    return panel_test, panel_train\n",
    "\n",
    "# ----------------------------\n",
    "# Load + features + split\n",
    "# ----------------------------\n",
    "tickers = TICKERS or list_tickers(DATA_DIR)\n",
    "print(f\"Using {len(tickers)} tickers: {tickers[:10]}{'...' if len(tickers)>10 else ''}\")\n",
    "\n",
    "d_long, d_idx, daily_pct_change, daily_close_px = prep_data(tickers, START, END, K=K_HOLD)\n",
    "X_train, y_train, X_test, y_test, data_train, data_test = split_train_test(d_idx, daily_close_px)\n",
    "\n",
    "# Keep non-scaled copies (as in your inspiration)\n",
    "X_train_nonscaled = X_train.copy()\n",
    "X_test_nonscaled  = X_test.copy()\n",
    "\n",
    "# Standardized versions (handy for other models)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled  = pd.DataFrame(scaler.transform(X_test),  columns=X_test.columns)\n",
    "\n",
    "# ============================\n",
    "# OLS\n",
    "# ============================\n",
    "lr = LinearRegression().fit(X_train_nonscaled, y_train)\n",
    "print(f\"OLS R2 train: {lr.score(X_train_nonscaled, y_train):.3f} | test: {lr.score(X_test_nonscaled, y_test):.3f}\")\n",
    "\n",
    "# ============================\n",
    "# Random Forest\n",
    "# ============================\n",
    "forest = RandomForestRegressor(\n",
    "    n_estimators=450, max_features=3, max_depth=4,\n",
    "    min_samples_split=3, min_samples_leaf=3,\n",
    "    n_jobs=-1, random_state=42\n",
    ")\n",
    "forest.fit(X_train_nonscaled, y_train)\n",
    "print(f\"RF  R2 train: {forest.score(X_train_nonscaled, y_train):.3f} | test: {forest.score(X_test_nonscaled, y_test):.3f}\")\n",
    "\n",
    "# (optional) light grid\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "param_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'max_features': [2, 3, 6],\n",
    "    'max_depth': [5, 15],\n",
    "    'min_samples_split': [5, 15],\n",
    "    'min_samples_leaf': [5, 15],\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid, n_jobs=-1, cv=3)\n",
    "grid.fit(X_train_nonscaled, y_train)\n",
    "print(\"RF grid best:\", grid.best_params_, \"| R2 test:\", grid.score(X_test_nonscaled, y_test))\n",
    "\n",
    "# ----------------------------\n",
    "# Convert predictions to panels\n",
    "# ----------------------------\n",
    "RF_results, _  = attach_preds(forest, X_train_nonscaled, X_test_nonscaled, data_train, data_test, 'RF')\n",
    "OLS_results, _ = attach_preds(lr,     X_train_nonscaled, X_test_nonscaled, data_train, data_test, 'OLS')\n",
    "\n",
    "# ----------------------------\n",
    "# Momentum comparison\n",
    "# ----------------------------\n",
    "tmp = ensure_date_column(data_test)\n",
    "date_col = find_date_col(tmp)\n",
    "MOM_7 = tmp.pivot_table(index=date_col, columns=\"Ticker\", values=\"J_7\")\n",
    "\n",
    "MOM_7_frame, MOM_7_cum, MOM_7_profit = MOM_Profit(MOM_7, K_HOLD, daily_close_px)\n",
    "RF_frame,  RF_cum,  RF_profit       = MOM_Profit(RF_results, K_HOLD, daily_close_px)\n",
    "OLS_frame, OLS_cum, OLS_profit      = MOM_Profit(OLS_results, K_HOLD, daily_close_px)\n",
    "\n",
    "print(f\"7/{K_HOLD} MOM | profit: {MOM_7_profit:.3f} | weekly r%: {MOM_7_frame.MomentumProfit.mean()*7:.3f} | sigma: {MOM_7_frame.MomentumProfit.std():.3f}\")\n",
    "print(f\"RF        | profit: {RF_profit:.3f} | weekly r%: {RF_frame.MomentumProfit.mean()*7:.3f} | sigma: {RF_frame.MomentumProfit.std():.3f}\")\n",
    "print(f\"OLS       | profit: {OLS_profit:.3f} | weekly r%: {OLS_frame.MomentumProfit.mean()*7:.3f} | sigma: {OLS_frame.MomentumProfit.std():.3f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Plot cumulative returns\n",
    "# ----------------------------\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(MOM_7_cum.MomentumProfit/100, label='MOM 7/K')\n",
    "ax.plot(RF_cum.MomentumProfit/100,    label='RF')\n",
    "ax.plot(OLS_cum.MomentumProfit/100,   label='OLS')\n",
    "ax.axhline(0, color='k', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Cumulative Return'); ax.legend()\n",
    "vals = ax.get_yticks(); ax.set_yticklabels([f\"{v:.0%}\" for v in vals])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Drawdowns\n",
    "# ----------------------------\n",
    "def DD(cum_series):\n",
    "    hw = cum_series.cummax()\n",
    "    dd = -((hw - cum_series) / hw).fillna(0)\n",
    "    return dd, dd.min()\n",
    "\n",
    "mom_dd, mom_mdd = DD(MOM_7_cum.MomentumProfit)\n",
    "rf_dd,  rf_mdd  = DD(RF_cum.MomentumProfit)\n",
    "ols_dd, ols_mdd = DD(OLS_cum.MomentumProfit)\n",
    "print(f\"Max DD — MOM:{mom_mdd:.3%} | RF:{rf_mdd:.3%} | OLS:{ols_mdd:.3%}\")\n",
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(10,8), sharex=True, sharey=True)\n",
    "for a, dd, ttl, col in zip(ax, [mom_dd, rf_dd, ols_dd], ['MOM 7/K','RF','OLS'], ['b','g','gray']):\n",
    "    a.plot(dd, color=col, linewidth=1); a.fill_between(dd.index, dd, 0, alpha=0.2, color=col); a.set_title(ttl)\n",
    "fig.text(0.01, 0.5, 'Drawdown', va='center', rotation='vertical')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
