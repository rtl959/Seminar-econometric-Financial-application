{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed6c1dd",
   "metadata": {},
   "source": [
    "Seminar - Econometrics in Financial Applicatios: Random forrest applicability as momentum trading strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceef45a",
   "metadata": {},
   "source": [
    "By Conrad Kromann & Tobias Dines Schlünssen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132dbe6",
   "metadata": {},
   "source": [
    "Credit for facilitating this paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90db629",
   "metadata": {},
   "source": [
    "Teachers: Frederik Findsen & Mads Thorndal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeda9af",
   "metadata": {},
   "source": [
    "Author of basecode and background paper: Mads Hemmingsen author of paper \"Momentum, Machine Learning, and Cryptocurrency\" (2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tænk over at have en renv:restore() function i toppen\n",
    "%pip install yfinance\n",
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cdb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download tickers from 15 biggest Drug manucatureers (volume above 1m)\n",
    "df = yf.download(\n",
    "    tickers= [\"LLY\", \"JNJ\", \"ABBV\", \"RHHBY\", \"AZN\", \"NVS\", \"NVO\", \"MRK\", \"AMGN\", \"PFE\", \"GILD\", \"SNY\", \"BMY\", \"GSK\", \"BIIB\", \"OGN\"],\n",
    "    start=\"2015-01-01\",\n",
    "    end=\"2025-01-01\",\n",
    "    interval=\"1d\",\n",
    "    auto_adjust= False,   # adjusts for splits/dividends\n",
    "    progress=False\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "output_file = \"drug_manufacturers_stock_data.csv\" \n",
    "df.to_csv(output_file) \n",
    "print(f\"\\n✅ Data successfully saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6af6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternativ way to download data in long format\n",
    "def yf_long (tickers, startdate, enddate, interval=\"1d\", auto_adjust=False, progress=False):\n",
    "    df = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=startdate,\n",
    "        end=enddate,\n",
    "        interval=interval,\n",
    "        auto_adjust=auto_adjust,\n",
    "        progress=progress\n",
    "    )\n",
    "\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df = df.stack(level=1).rename_axis(['Date', 'Ticker']).reset_index()\n",
    "    else:\n",
    "        df = df.reset_index()\n",
    "        df[\"Ticker\"] = tickers if isinstance(tickers, str) else tickers[0]\n",
    "    return df\n",
    "\n",
    "tickers= [\"LLY\", \"JNJ\", \"ABBV\", \"RHHBY\", \"AZN\", \"NVS\", \"NVO\", \"MRK\", \"AMGN\", \"PFE\", \"GILD\", \"SNY\", \"BMY\", \"GSK\", \"BIIB\", \"OGN\"]\n",
    "start=\"2015-01-01\"\n",
    "end=\"2025-01-01\"\n",
    "\n",
    "data= yf_long(tickers, start, end)\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "csv_filename = \"pharma_stocks_2015_2025.csv\"\n",
    "data.to_csv(csv_filename, index=False)\n",
    "print(f\"\\n✅ Data saved to '{csv_filename}' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crypto download (skal ikke bruges)\n",
    "\n",
    "#tickers = ['BTC', 'ETH', 'USDT', 'XRP', 'DOGE', 'LTC', 'XLM', 'XMR', 'XEM']\n",
    "#start = datetime.datetime(2015, 8, 8) # Total set\n",
    "#end = datetime.datetime(2022, 4, 22) # Total set\n",
    "\n",
    "#def get(tickers, startdate, enddate):\n",
    "   # def data(ticker):\n",
    "       # full = pd.read_csv('./Master Thesis data/ticker data/New tickers/With volume/' + ticker + '.CSV', parse_dates=[3], index_col='Date')\n",
    "        #full = full.loc[start:end]\n",
    "\n",
    "        #return full\n",
    "    #datas = map(data, tickers)\n",
    "    #full_concat = pd.concat(datas, keys=tickers, names=['Ticker', 'Date'])\n",
    "   # full_concat = full_concat.drop(['SNo', 'Name', 'Ticker', 'Marketcap'], axis = 1)\n",
    "  #  full_concat['Adj Close'] = full_concat.Close\n",
    "\n",
    "  #  return full_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(tickers, start, end, K):\n",
    "    all_data = get(tickers, startdate=start, enddate=end)\n",
    "    daily_close_px = all_data[['Adj Close']].reset_index().pivot('Date', 'Ticker', 'Adj Close')\n",
    "    daily_pct_change = daily_close_px.pct_change()\n",
    "    #daily_pct_change = daily_pct_change.dropna(axis='index')\n",
    "\n",
    "    dly_vars = all_data[['Adj Close', 'Volume']].reset_index()\n",
    "\n",
    "    dly_vars = pd.DataFrame({'Ticker': dly_vars['Ticker'],\n",
    "                             'Date': dly_vars['Date'],\n",
    "                             'Price': dly_vars['Adj Close'],\n",
    "                             'Volume': dly_vars['Volume'],\n",
    "                             't-1': 0,\n",
    "                             'mkt_t-1': 0,\n",
    "                             'target': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb221802",
   "metadata": {},
   "source": [
    "Creating momentum varables for market and asset and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating momentum variable for market\n",
    "\n",
    "#Market 1 . Return of market t-skip //reversal ************\n",
    "mkt_ret = daily_pct_change.mean(axis=1)\n",
    "#Market 2 . Cumulative return of market t-T to t-skip //momentum ************\n",
    "mkt_cum_3 = (mkt_ret + 1).shift(1).rolling(3).apply(np.prod) - 1\n",
    "mkt_cum_7 = (mkt_ret + 1).shift(1).rolling(7).apply(np.prod) - 1\n",
    "mkt_cum_15 = (mkt_ret + 1).shift(1).rolling(15).apply(np.prod) - 1\n",
    "mkt_cum_30 = (mkt_ret + 1).shift(1).rolling(30).apply(np.prod) - 1\n",
    "#Market 3 . standard deviation of the market from t-T to t-skip //volatility\n",
    "mkt_std_3 = (mkt_ret + 1).shift(1).rolling(3).std()\n",
    "mkt_std_7 = (mkt_ret + 1).shift(1).rolling(7).std()\n",
    "mkt_std_15 = (mkt_ret + 1).shift(1).rolling(15).std()\n",
    "mkt_std_30 = (mkt_ret + 1).shift(1).rolling(30).std()\n",
    "mkt_ret = mkt_ret.shift(1)\n",
    "\n",
    "#Asset specific momentum\n",
    "for ticker in tickers:\n",
    "    # ticker 1 . reversal t-1\n",
    "    dly_vars.loc[dly_vars.ticker == ticker, 't-1'] = dly_vars.loc[dly_vars.Ticker == ticker, 'Price'].pct_change()\n",
    "    # ticker 2.1 . momentum J=3\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_3'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(2).rolling(3 - 1).apply(np.prod) - 1\n",
    "    # # ticker 2.2 . momentum J=7\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_7'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(7 - 1).apply(np.prod) - 1\n",
    "    # # ticker 2.3 . momentum J=15\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_15'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(15 - 1).apply(np.prod) - 1\n",
    "    # # ticker 2.4 . momentum J=30\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_30'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(30 - 1).apply(np.prod) - 1\n",
    "    # ticker 3 . volatility t-7:t-1\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_std_3'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(3 - 1).std()\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_std_7'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(7 - 1).std()\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_std_15'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(15 - 1).std()\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'J_std_30'] = (dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] + 1).shift(\n",
    "        1).rolling(30 - 1).std()\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 't-1'] = dly_vars.loc[dly_vars.Ticker == ticker, 't-1'].shift(1)\n",
    "\n",
    "    \n",
    "    #mkt vars\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_t-1'] = mkt_ret.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_ret_3'] = mkt_cum_3.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_ret_7'] = mkt_cum_7.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_ret_15'] = mkt_cum_15.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_ret_30'] = mkt_cum_30.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_std_3'] = mkt_std_3.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_std_7'] = mkt_std_7.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_std_15'] = mkt_std_15.values\n",
    "    dly_vars.loc[dly_vars.Ticker == ticker, 'mkt_J_std_30'] = mkt_std_30.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622536d0",
   "metadata": {},
   "source": [
    "Creating target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # target returns\n",
    "    for ticker in tickers:\n",
    "        # target\n",
    "        dly_vars.loc[dly_vars.Ticker == ticker, 'target'] = \\\n",
    "            dly_vars.loc[dly_vars.Ticker == ticker, 'Price'].shift(-K) / dly_vars.loc[dly_vars.Ticker == ticker, 'Price'] - 1\n",
    "\n",
    "    # drop nan\n",
    "    dly_vars = dly_vars.dropna(axis = 'index')\n",
    "    dly_target = dly_vars.target\n",
    "    # dly_vars = dly_vars.drop(['target'], axis = 1)\n",
    "\n",
    "    # target = dly_vars.Price / dly_vars.Price.shift(7)-1\n",
    "    # target = target.shift(-7)\n",
    "    # target = target.iloc[:-7]\n",
    "    dly_target = np.ravel(dly_target)\n",
    "    # returns over past J days\n",
    "\n",
    "    dly_vars = dly_vars.sort_values(by='Date').reset_index(drop=True)\n",
    "    dly_data = dly_vars\n",
    "    dly_vars = dly_vars.drop(['Ticker', 'Date'], axis = 1)\n",
    "    dly_data = dly_data.set_index(dly_data.Date)\n",
    "\n",
    "    # MARKET FEATURES\n",
    "    dly_data = dly_data.drop(['mkt_t-1', 'mkt_J_ret_3', 'mkt_J_ret_7', 'mkt_J_ret_15', 'mkt_J_ret_30', 'mkt_J_std_3',\n",
    "                              'mkt_J_std_7', 'mkt_J_std_15', 'mkt_J_std_30'], axis=1)\n",
    "    #dly_data = dly_data.drop(['mkt_J_ret_3', 'mkt_J_ret_15', 'mkt_J_ret_30', 'mkt_J_std_3',\n",
    "    #                        'mkt_J_std_15', 'mkt_J_std_30'], axis=1)\n",
    "    # ticker features\n",
    "    #dly_data = dly_data.drop(['t-1', 'J_3', 'J_7', 'J_15', 'J_30', 'J_std_3',\n",
    "    #                          'J_std_7', 'J_std_15', 'J_std_30'], axis=1)\n",
    "\n",
    "    return dly_vars, dly_data, daily_pct_change, daily_close_px, dly_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66b325",
   "metadata": {},
   "source": [
    "Making Training/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dly_data, daily_close_px):\n",
    "    start_new = dly_data.Date.index[0]\n",
    "    end_new = dly_data.index[-1]\n",
    "    dates = daily_close_px.reset_index()\n",
    "    dates = dates.set_index(dates.Date)\n",
    "    dates = dates.Date.loc[start_new:end_new]\n",
    "\n",
    "    # Chronological split\n",
    "    test_size = int(len(dates) * 0.2)\n",
    "    dates_train = dates[:-test_size]\n",
    "    dates_test = dates[-test_size:]\n",
    "\n",
    "    X_train = dly_data.loc[dates_train.values, :]\n",
    "    X_train_old = X_train\n",
    "    data_train = X_train\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_train = X_train.drop(['Ticker', 'Date'], axis=1)\n",
    "    y_train = X_train['target']\n",
    "    X_train = X_train.drop(['target'], axis=1)\n",
    "\n",
    "    X_test = dly_data.loc[dates_test.values, :]\n",
    "    data_test = X_test\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    X_test = X_test.drop(['Ticker', 'Date'], axis=1)\n",
    "    y_test = X_test.target\n",
    "    X_test = X_test.drop(['target'], axis=1)\n",
    "\n",
    "    # Drop Price\n",
    "    X_train = X_train.drop(['Price'], axis=1)\n",
    "    X_test = X_test.drop(['Price'], axis=1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3787d0",
   "metadata": {},
   "source": [
    "Full backtesting of momentum strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ae089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum(pred_r, date, K):\n",
    "    ret = pred_r.loc[date].reset_index()\n",
    "    ret['quantile'] = pd.qcut(ret.iloc[:,1].rank(method='first'), 3, labels=False)\n",
    "\n",
    "    winners = ret[ret['quantile'] == 2]\n",
    "    losers = ret[ret['quantile'] == 0]\n",
    "\n",
    "    winnerret = daily_close_px.loc[date + relativedelta(days=K), daily_close_px.columns.isin(winners.Ticker)] / daily_close_px.loc[date, daily_close_px.columns.isin(winners.Ticker)] - 1\n",
    "    loserret = daily_close_px.loc[date + relativedelta(days=K), daily_close_px.columns.isin(losers.Ticker)] / daily_close_px.loc[date, daily_close_px.columns.isin(losers.Ticker)] - 1\n",
    "\n",
    "    Momentumprofit = winnerret.mean() - loserret.mean()\n",
    "\n",
    "    return Momentumprofit\n",
    "\n",
    "def MOM_Profit(returns, K):\n",
    "    profits = []\n",
    "    dates = []\n",
    "    # run profit\n",
    "    for date in returns.index[:-K]:\n",
    "        profits.append(Momentum(returns, date, K))\n",
    "        dates.append(date)\n",
    "\n",
    "    frame = pd.DataFrame({'MomentumProfit': profits}, index=dates)\n",
    "    cum_frame = frame.cumsum() * 100\n",
    "    profit = frame.MomentumProfit.sum() * 100\n",
    "\n",
    "    return frame, cum_frame, profit\n",
    "\n",
    "def plot_feature_importances_cancer(model, name):\n",
    "    importance = model.feature_importances_\n",
    "    columns = X_train.columns\n",
    "    Graph = pd.Series(importance, columns)\n",
    "    #Graph.sort_values()\n",
    "    Graph.plot.barh(color='red')\n",
    "    plt.title('Feature importances for' + name)\n",
    "    plt.show()\n",
    "\n",
    "def PURE_MOM():\n",
    "    MOM_3 = dly_data.pivot(index='Date', columns='Ticker', values='J_3')\n",
    "    MOM_3_frame, MOM_3_cum_frame, MOM_3_profit = MOM_Profit(MOM_3, 7)\n",
    "    print('3/7 profit: ', MOM_3_profit, 'weekly r%: ', MOM_3_frame.MomentumProfit.mean() * 7, 'SD: ', MOM_3_frame.MomentumProfit.std())\n",
    "\n",
    "    MOM_7 = dly_data.pivot(index='Date', columns='Ticker', values='J_7')\n",
    "    MOM_7_frame, MOM_7_cum_frame, MOM_7_profit = MOM_Profit(MOM_7, 7)\n",
    "    print('7/7 profit: ', MOM_7_profit, 'weekly r%: ', MOM_7_frame.MomentumProfit.mean() * 7, 'SD: ', MOM_7_frame.MomentumProfit.std())\n",
    "\n",
    "    MOM_15 = dly_data.pivot(index='Date', columns='Ticker', values='J_15')\n",
    "    MOM_15_frame, MOM_15_cum_frame, MOM_15_profit = MOM_Profit(MOM_15, 7)\n",
    "    print('15/7 profit: ', MOM_15_profit, 'weekly r%: ', MOM_15_frame.MomentumProfit.mean() * 7, 'SD: ', MOM_15_frame.MomentumProfit.std())\n",
    "\n",
    "    MOM_30 = dly_data.pivot(index='Date', columns='Ticker', values='J_30')\n",
    "    MOM_30_frame, MOM_30_cum_frame, MOM_30_profit = MOM_Profit(MOM_30, 7)\n",
    "    print('30/7 profit: ', MOM_30_profit, 'weekly r%: ', MOM_30_frame.MomentumProfit.mean() * 7, 'SD: ', MOM_30_frame.MomentumProfit.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f3f92",
   "metadata": {},
   "source": [
    "Preparing and scaling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables\n",
    "dly_vars, dly_data, daily_pct_change, daily_close_px, dly_target = prep_data(tickers, start, end, K=7)\n",
    "X_train, y_train, X_test, y_test, data_train, data_test = train_test_split(dly_data, daily_close_px)\n",
    "\n",
    "# Scale datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_MinMax = MinMaxScaler()\n",
    "scaler_Standard = StandardScaler()\n",
    "\n",
    "scaler_MinMax.fit(X_train)\n",
    "X_train_scaled_MinMax = pd.DataFrame(data=scaler_MinMax.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled_MinMax = pd.DataFrame(data=scaler_MinMax.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "scaler_Standard.fit(X_train)\n",
    "X_train_scaled_Standard = pd.DataFrame(data=scaler_Standard.transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled_Standard = pd.DataFrame(data=scaler_Standard.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "X_train_nonscaled = X_train\n",
    "X_test_nonscaled = X_test\n",
    "\n",
    "X_train = X_train_scaled_Standard\n",
    "X_test = X_test_scaled_Standard\n",
    "#***X_train = X_train_nonscaled\n",
    "#***X_test = X_test_nonscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3805606",
   "metadata": {},
   "source": [
    "Training OLS-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train_nonscaled, y_train)\n",
    "print(f\"Accuracy on training set (OLS): {(lr.score(X_train_nonscaled, y_train) * 100):.2f} %\")\n",
    "print(f\"Accuracy on test set (OLS): {(lr.score(X_test_nonscaled, y_test) * 100):.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977c31a",
   "metadata": {},
   "source": [
    "Random forrest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "forest = RandomForestRegressor(n_estimators=450, max_features=3, max_depth=4, min_samples_split=3, min_samples_leaf =3, n_jobs=-1) #optimized params\n",
    "forest.fit(X_train_nonscaled, y_train)\n",
    "print(f\"Accuracy on training set (rf): {(forest.score(X_train_nonscaled, y_train) * 100):.2f} %\")\n",
    "print(f\"Accuracy on test set (rf): {(forest.score(X_test_nonscaled, y_test) * 100):.2f} %\")\n",
    "plot_feature_importances_cancer(forest, ' Random Forest')\n",
    "#Paramerter optimization\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(5, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_nonscaled, y_train)\n",
    "print(f\"Accuracy on training set (rf_random): {(rf_random.score(X_train_nonscaled, y_train) * 100):.3f} %\")\n",
    "print(f\"Accuracy on test set (rf_random): {(rf_random.score(X_test_nonscaled, y_test) * 100):.3f} %\")\n",
    "\n",
    "plot_feature_importances_cancer(forest, ' Random Forest')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'max_features': [2, 3, 6],\n",
    "    'max_depth': [5, 15, 30],\n",
    "    'min_samples_split': [5, 15, 30],\n",
    "    'min_samples_leaf': [5, 15, 30],\n",
    "    'max_leaf_nodes' : [5, 15, 30],\n",
    "    'n_jobs' : [-1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(rf, param_grid, n_jobs= -1, cv=5)\n",
    "grid.fit(X_train_nonscaled, y_train)\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(f\"Accuracy on training set (rf_grid): {(grid.score(X_train_nonscaled, y_train) * 100):.3f} %\")\n",
    "print(f\"Accuracy on test set (rf_grid): {(grid.score(X_test_nonscaled, y_test) * 100):.3f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
